version: "3"
services:
  prometheus:
    image: prom/prometheus:v2.4.3
    container_name: 'prometheus'
    volumes:
    - ./config/:/etc/prometheus/
    - ./config/localtime:/etc/localtime:ro
    ports:
     - '9090:9090'
  grafana:
    image: grafana/grafana:5.2.4
    container_name: 'grafana'
    ports:
      - '3000:3000'
    volumes:
      - ./grafana/config/grafana.ini:/etc/grafana/grafana.ini  #grafana报警邮件配置
      - ./grafana/provisioning/:/etc/grafana/provisioning/  #配置grafana的prometheus数据源
      - ./config/localtime:/etc/localtime:ro
    env_file:
      - ./grafana/config.monitoring  #grafana登录配置
    depends_on:
      - prometheus  #grafana需要在prometheus之后启动
  elasticsearch:
    image: elasticsearch:6.4.0
    container_name: elasticsearch
    environment:
      - "cluster.name=elasticsearch" #设置集群名称为elasticsearch
      - "discovery.type=single-node" #以单一节点模式启动
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m" #设置使用jvm内存大小，稍微配置大点，不然有可能启动不成功
    volumes:
      - ./data/elasticsearch/plugins:/usr/share/elasticsearch/plugins #插件文件挂载
      - ./data/elasticsearch/data:/usr/share/elasticsearch/data #数据文件挂载
    ports:
      - 9200:9200
      - 9300:9300
  kibana:
    image: kibana:6.4.0
    container_name: kibana
    links:  #同一个compose文件管理的服务可以直接用服务名访问，如果要给服务取别名则可以用links实现，如下面的es就是elasticsearch服务的别名
      - elasticsearch:es #可以用es这个域名访问elasticsearch服务
    depends_on:
      - elasticsearch #kibana在elasticsearch启动之后再启动
    environment:
      - "elasticsearch.hosts=http://es:9200" #设置访问elasticsearch的地址
    ports:
      - 5601:5601
  redis:
    image: redis
    container_name: redis
    command: redis-server /etc/redis/redis.conf --appendonly yes
    volumes:
      - ./data/redis/data/:/data/
      - ./redis/redis.conf:/etc/redis/redis.conf
      - ./config/localtime:/etc/localtime:ro
    ports:
      - 6379:6379
  redis_exporter:
    image: oliver006/redis_exporter:v1.6.1-arm
    container_name: redis_exporter
    volumes:
      - ./config/localtime:/etc/localtime:ro
    environment:
      - "REDIS_ADDR=redis://192.168.3.9:6379"
      - "REDIS_PASSWORD=886887"
    ports:
      - 9121:9121
    depends_on:
      - redis
  mysql:
    image: mysql/mysql-server:5.7
    container_name: mysql
    command: mysqld --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci --default-authentication-plugin=mysql_native_password
#    restart: always
    environment:
      - "TZ=Asia/Shanghai" # 设置容器时区与宿主机保持一致
      - "MYSQL_ROOT_PASSWORD=886887"
    volumes:
      - ./config/localtime:/etc/localtime:ro # 设置容器时区与宿主机保持一致
      - ./data/mysql/:/var/lib/mysql # 映射数据库保存目录到宿主机，防止数据丢失
      - ./mysql:/etc/mysql # 映射数据库配置文件
    ports:
      - 3306:3306
  mysql_exporter:
    image: prom/mysqld-exporter
    container_name: mysql_exporer
    volumes:
      - ./config/localtime:/etc/localtime:ro
    ports:
      - 9104:9104
    environment:
      - "DATA_SOURCE_NAME=root:886887@(192.168.3.9:3306)/final"
    depends_on:
      - mysql
  jenkins:
    container_name: webjenkins
    image: 'jenkins/jenkins:lts'
    user: root
    restart: always
    environment:
      - TZ=Asia/Shanghai
    ports:
      - '3083:8080'
      - '50001:50000'
    volumes:
      - ./jenkins-data:/var/jenkins_home:z
      - ./jenkins-data/docker.sock:/var/run/docker.sock
    network_mode: "bridge"
  zk:
    image: zookeeper:3.5
    restart: always
    container_name: zk
    hostname: zk
    ports:
      - "2181:2181"
    volumes:
      - ./config/localtime:/etc/localtime
      - ./data/zk/zoo1/data:/data
      - ./data/zk/zoo1/datalog:/datalog
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zk:2888:3888;2181
  kafka:
    image: confluentinc/cp-kafka:5.5.1
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092"
      - 9999:9999
    environment:
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka:19092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: zk:2181
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      JMX_PORT: 9999
    volumes:
      - ./data/zk-single-kafka-single/kafka1/data:/var/lib/kafka/data
    depends_on:
      - zk
  kafka-manager:
    container_name: kafka-manager
    image: sheepkiller/kafka-manager                # 镜像：开源的web管理kafka集群的界面
    links:
      - zk
      - kafka
    environment:
      ZK_HOSTS: zk:2181
      KAFKA_BROKERS: kafka:9092
    volumes:
    - ./config/localtime:/etc/localtime
    ports:
      - "9001:9000"
  ngrok:
    image: shkoliar/ngrok:latest
    ports:
      - 4551:4551
    environment:
      - DOMAIN=192.168.0.109
      - PORT=8999
      - PROTOCOL=tcp
  nginx:
    restart: always
    image: nginx:1.11.6-alpine
    ports:
      - 8080:80
      - 80:80
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/.htpasswd:/etc/nginx/.htpasswd
      - ./data/nginx/log:/var/log/nginx
      - ./www:/var/www
      - /etc/letsencrypt:/etc/letsencrypt
#  live:
#    image: live
#    build: ./live
#    container_name: live
#    ports:
#      - 8887:8887
#      - 8886:8886
#    volumes:
#      - /etc/localtime:/etc/localtime:ro
#      - ./live/log:/usr/local/var/log